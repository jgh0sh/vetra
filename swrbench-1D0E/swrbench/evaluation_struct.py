import argparse
from concurrent.futures import ThreadPoolExecutor, as_completed
import json
import os
import re
import threading
import numpy as np
import logging
import random
from tqdm import tqdm
from utils import run_chat
from loguru import logger

CHANGE_TYPE_TEXT_MAP = {
    'E.1': 'Documentation',
    'E.1.1': 'Textual Changes',
    'E.1.2': 'Language Features',
    'E.2': 'Visual Representation',
    'E.3': 'Structure',
    'E.3.1': 'Organization',
    'E.3.2': 'Solution Approach',
    'F.1': 'Interface',
    'F.2': 'Logic',
    'F.3': 'Resource',
    'F.4': 'Check',
    'F.5': 'Support',
    'F.6': 'Larger Defects',
    'F.': 'Functional',
    'E.': 'Evolvability'
}

EVAL_CLEAN_PROMPT = """<Role>
You are an objective Evaluation Assistant specialized in analyzing code review predictions.
</Role>

<OverallTask>
Your task is to evaluate a predicted code review provided for a pull request (PR) that is known to be **clean** (i.e., it introduces no actual defects according to the ground truth).
Your primary goals are:
1.  Determine if the predicted review correctly identifies the PR as clean/good.
2.  If the predicted review raises any issues (potential false positives), identify, categorize, and assess the **inherent severity** of each potential false positive using the provided definitions.
</OverallTask>

<InputContext>
    <PullRequestDetails>
        <Title>{pr_title}</Title>
        <Description>{pr_statement}</Description>
        <Status>The pull request is confirmed to be good and clean, ready to merge.</Status>
    </PullRequestDetails>

    <PredictedReview>
        <Content>
{pred_review}
        </Content>
        <Note>This is the review generated by the system being evaluated.</Note>
    </PredictedReview>
</InputContext>

<ReferenceData>
    <ChangeCategoryDefinitions>
        <Description>Use these categories to classify any issues identified in the <PredictedReview>.</Description>
        <Categories>
        - **E. Evolvability Changes**: Modifications improving code readability, maintainability, or overall structure without altering core functionality.
            - **E.1 Documentation**: Enhancements to code comprehension for developers through documentation elements.
                - *E.1.1 Textual Changes*: Adjustments to comments (e.g., adding, correcting, clarifying) or identifier names (variables, functions, classes) for better clarity and consistency.
                - *E.1.2 Language Features*: Utilizing language-specific constructs (e.g., `final` in Java, type annotations, access modifiers) primarily to convey developer intent, constraints, or information, rather than for functional impact.
            - **E.2 Visual Representation**: Modifications to code formatting and layout, such as indentation, spacing, line breaks, or bracket placement, to improve visual clarity and adhere to style conventions.
            - **E.3 Structure**: Changes affecting the organization or implementation strategy of the code without changing its external behavior.
                - *E.3.1 Organization*: Reorganizing code elements, such as removing dead (unused) code, moving functions or classes to more appropriate locations, or restructuring files/packages for better modularity.
                - *E.3.2 Solution Approach*: Modifying the internal implementation details or algorithms (e.g., refactoring for clarity/efficiency, updating function usage to newer patterns), or adding supporting code like tests, without altering the observable functionality.

        - **F. Functional Changes**: Corrections addressing errors in the code's behavior, output, or interaction with other system parts.
            - **F.1 Interface**: Fixes related to how different code components interact, including incorrect method calls, wrong parameter types/values, violated API contracts, or incorrect event handling.
            - **F.2 Logic**: Corrections to errors in algorithms, conditional statements (if/else), loops, computations, or other logical constructs leading to incorrect behavior.
            - **F.3 Resource**: Fixes concerning the management of data, variables, or system resources, including initialization errors, memory leaks, improper resource release/acquisition, or incorrect data manipulation (e.g., concurrency issues).
            - **F.4 Check**: Adding or modifying validation or checks (e.g., null checks, boundary checks, state validation) for variables, parameters, or function return values to handle potential errors or invalid states correctly.
            - **F.5 Support**: Corrections related to the interaction with external systems, libraries, frameworks, or APIs (e.g., incorrect usage, adapting to API changes, version incompatibilities).
            - **F.6 Larger Defects**: Significant functional fixes that often span multiple files or components, address incompletely implemented features, fix major inconsistencies (like GUI behavior), or require broader system knowledge. These might not always be fully resolved within the same PR.
        </Categories>
    </ChangeCategoryDefinitions>

    <SeverityScale>
        <Description>Use this scale to evaluate the inherent severity of potential false positives identified in the <PredictedReview>.</Description>
        <Scale>
            *   **1-2: Trivial** (e.g., minor comment typo, inconsequential whitespace change suggestion).
            *   **3-4: Minor** (e.g., slightly inefficient code pattern suggestion, unclear logging message, minor style deviation).
            *   **5-6: Moderate** (e.g., suggests handling a specific but unlikely edge case, potential minor performance optimization).
            *   **7-8: Major** (e.g., implies potentially incorrect logic, suggests a fix for a non-existent crash in a non-core function).
            *   **9-10: Critical** (e.g., alleges a core functionality crash, claims a security vulnerability where none exists).
        </Scale>
    </SeverityScale>
</ReferenceData>

<EvaluationInstructions>
Follow these steps precisely:

1.  **Analyze Predicted Review Content:**
    *   Carefully read the content within `<InputContext><PredictedReview><Content>`.
    *   Determine if the review **explicitly states or strongly implies** the PR is clean, good, approved, or ready to merge (e.g., using phrases like 'LGTM', 'Approved', 'Looks good', 'No issues found').
    *   Record this determination as `YES` or `NO` for the `identified_as_good` field in the output. **Base this assessment solely on such explicit statements, even if minor suggestions are also present.**
    *   Identify *all* potential issues, defects, suggestions, or specific points for improvement mentioned in the `<InputContext><PredictedReview><Content>`.

2.  **Identify and Assess Predicted Issues (Potential False Positives):**
    *   This step applies only if the `<InputContext><PredictedReview><Content>` raises *any* specific issues, comments, or suggestions. If none are raised, skip to the output step and provide an empty list for `predicted_points`.
    *   For each distinct point identified in Step 1:
        *   Assign a unique sequential ID: `PRED-POINT-1`, `PRED-POINT-2`, etc.
        *   Extract the core original description of the point directly from the `<InputContext><PredictedReview><Content>`.
        *   Determine the most appropriate category for the point by referencing the definitions in `<ReferenceData><ChangeCategoryDefinitions>`. Use the specific code (e.g., `E.1.1`, `F.2`).
        *   Evaluate the **inherent severity** of the *potential* issue raised by the point, assuming it *were* a real issue, based on its nature. Use the scale provided in `<ReferenceData><SeverityScale>`.
        *   Record the ID, description, change category, and severity score for each identified point.

</EvaluationInstructions>

<OutputFormat>
Provide your evaluation **strictly** in the following JSON format. Do not include any text outside the JSON structure.

```json
{{
  "identified_as_good": "[YES or NO]",
  "pred_points": [
    // Include one object for each identified issue.
    // If no issues were raised in the Predicted Review, return an empty array: []
    {{
      "id": "PRED-POINT-1",
      "description": "[Extract the original description of the first point identified from the <InputContext><PredictedReview><Content>]",
      "change_category": "[Assign the category code (e.g., E.1.1, F.2) from <ReferenceData><ChangeCategoryDefinitions>]",
      "severity_score": "[Assign the score (1-10) based on the criteria in <ReferenceData><SeverityScale>]"
    }},
    {{
      "id": "PRED-POINT-2",
      "description": "[Extract the original description of the second point identified from the <InputContext><PredictedReview><Content>]",
      "change_category": "[Assign the category code (e.g., E.1.1, F.2) from <ReferenceData><ChangeCategoryDefinitions>]",
      "severity_score": "[Assign the score (1-10) based on the criteria in <ReferenceData><SeverityScale>]"
    }}
    // ... continue for all identified predicted issues ...
  ]
}}
"""


EVAL_CHANGE_PROMPT = """<Role>
You are an objective Evaluation Assistant specialized in analyzing code review predictions against ground truth information.
</Role>

<OverallTask>
Your task is to evaluate a predicted code review based on how accurately it identifies the actual **changes or issues** present in a pull request (PR). You will compare the points raised in the predicted review against a ground truth list of confirmed changes/issues introduced by the PR.
</OverallTask>

<InputContext>
    <PullRequestDetails>
        <Note>This PR is known to contain the changes/issues listed in GroundTruthChanges.</Note>
        <Title>{pr_title}</Title>
        <Description>{pr_statement}</Description>
        <GroundTruthReviewContent>
        {ground_truth_reviews}
        </GroundTruthReviewContent>
    </PullRequestDetails>

    <GroundTruthChanges>
        <Description>This section lists the actual changes/issues confirmed to be present in the pull request. Each item has an ID, a change category, and a description.</Description>
        <Content>
{changes_description}
        </Content>
    </GroundTruthChanges>

    <PredictedReview>
        <Content>
{pred_review}
        </Content>
        <Note>This is the review generated by the system being evaluated.</Note>
    </PredictedReview>
</InputContext>

<ReferenceData>
    <ChangeCategoryDefinitions>
        <Description>Use these categories to classify any points identified in the <PredictedReview> and to understand the categories provided in <GroundTruthChanges>.</Description>
        <Categories>
        - **E. Evolvability Changes**: Modifications improving code readability, maintainability, or overall structure without altering core functionality.
            - **E.1 Documentation**: Enhancements to code comprehension for developers through documentation elements.
                - *E.1.1 Textual Changes*: Adjustments to comments (e.g., adding, correcting, clarifying) or identifier names (variables, functions, classes) for better clarity and consistency.
                - *E.1.2 Language Features*: Utilizing language-specific constructs (e.g., `final` in Java, type annotations, access modifiers) primarily to convey developer intent, constraints, or information, rather than for functional impact.
            - **E.2 Visual Representation**: Modifications to code formatting and layout, such as indentation, spacing, line breaks, or bracket placement, to improve visual clarity and adhere to style conventions.
            - **E.3 Structure**: Changes affecting the organization or implementation strategy of the code without changing its external behavior.
                - *E.3.1 Organization*: Reorganizing code elements, such as removing dead (unused) code, moving functions or classes to more appropriate locations, or restructuring files/packages for better modularity.
                - *E.3.2 Solution Approach*: Modifying the internal implementation details or algorithms (e.g., refactoring for clarity/efficiency, updating function usage to newer patterns), or adding supporting code like tests, without altering the observable functionality.

        - **F. Functional Changes**: Corrections addressing errors in the code's behavior, output, or interaction with other system parts.
            - **F.1 Interface**: Fixes related to how different code components interact, including incorrect method calls, wrong parameter types/values, violated API contracts, or incorrect event handling.
            - **F.2 Logic**: Corrections to errors in algorithms, conditional statements (if/else), loops, computations, or other logical constructs leading to incorrect behavior.
            - **F.3 Resource**: Fixes concerning the management of data, variables, or system resources, including initialization errors, memory leaks, improper resource release/acquisition, or incorrect data manipulation (e.g., concurrency issues).
            - **F.4 Check**: Adding or modifying validation or checks (e.g., null checks, boundary checks, state validation) for variables, parameters, or function return values to handle potential errors or invalid states correctly.
            - **F.5 Support**: Corrections related to the interaction with external systems, libraries, frameworks, or APIs (e.g., incorrect usage, adapting to API changes, version incompatibilities).
            - **F.6 Larger Defects**: Significant functional fixes that often span multiple files or components, address incompletely implemented features, fix major inconsistencies (like GUI behavior), or require broader system knowledge. These might not always be fully resolved within the same PR.
        </Categories>
    </ChangeCategoryDefinitions>

    <SeverityScale>
        <Description>Use this scale to evaluate the inherent severity of the points identified in the <PredictedReview>.</Description>
        <Scale>
            *   **1-2: Trivial** (e.g., minor comment typo, inconsequential whitespace change suggestion).
            *   **3-4: Minor** (e.g., suggestion for slightly inefficient code, unclear logging message, minor style deviation).
            *   **5-6: Moderate** (e.g., identifying a specific edge case error, a noticeable performance issue suggestion).
            *   **7-8: Major** (e.g., identifying incorrect logic, a crash in a non-core function).
            *   **9-10: Critical** (e.g., identifying a core functionality crash, a security vulnerability).
        </Scale>
    </SeverityScale>
</ReferenceData>

<EvaluationInstructions>
Follow these steps precisely:

1.  **Analyze Predicted Review Content:**
    *   Carefully read the content within `<InputContext><PredictedReview><Content>`.
    *   Determine if the review **explicitly states or strongly implies** the PR is clean, good, approved, or ready to merge (e.g., using phrases like 'LGTM', 'Approved', 'Looks good', 'No issues found').
    *   Record this determination as `YES` or `NO` for the `identified_as_good` field in the output. **Base this assessment solely on such explicit statements, even if minor suggestions are also present.**

2.  **Identify and Analyze Predicted Points:**
    *   Carefully read the content within `<InputContext><PredictedReview><Content>`.
    *   Identify *each distinct suggestion, issue, potential defect, or point for change* mentioned in the review.
    *   For each identified point:
        *   Assign a unique sequential ID: `PRED-POINT-1`, `PRED-POINT-2`, etc.
        *   Extract the core original description of the point directly from the `<InputContext><PredictedReview><Content>`.
        *   Determine the most appropriate change category for the point by referencing the definitions in `<ReferenceData><ChangeCategoryDefinitions>`. Use the specific code (e.g., `E.1.1`, `F.2`).
        *   Evaluate the **inherent severity** of the *potential* issue raised by the point, based on its nature (independent of whether it matches a ground truth change). Use the scale provided in `<ReferenceData><SeverityScale>` (1-10).
        *   Record the ID, description, change category, and severity score for each identified point.

3.  **Match Ground Truth Changes to Predicted Points:**
    *   For *each* change listed in `<InputContext><GroundTruthChanges><Content>` (e.g., `GT-CHANGE-1`, `GT-CHANGE-2`, etc., assuming IDs are provided in the input):
        *   Review its description and change category.
        *   Determine if any of the predicted points identified in Step 1 (`PRED-POINT-1`, `PRED-POINT-2`, etc.) successfully **hit** this ground truth change. A hit occurs if a predicted point *substantially and correctly identifies the core essence* of the ground truth change based on their **semantic meaning and description**. The assigned severity or exact wording doesn't need to match perfectly.
        *   If multiple predicted points seem relevant to a single ground truth change, choose the predicted point that provides the *most accurate and complete semantic* description of the ground truth change as the primary hit. A ground truth change should only be marked as hit by *one* predicted point at most.
        *   Record `Hit: YES` or `Hit: NO` for the ground truth change.
        *   If `Hit: YES`, record the ID (e.g., `PRED-POINT-1`) of the specific predicted point that hit it in the `Hit by:` field.
        *   If `Hit: NO`, record `Hit by: N/A`.

</EvaluationInstructions>

<OutputFormat>
Provide your evaluation **strictly** in the following JSON format. Do not include any text outside the JSON structure. Ensure all ground truth changes from the input are listed in the `gt_changes` array, in the same order they appeared in the input.

```json
{{
  "identified_as_good": "[YES or NO]",
  "pred_points": [
    // Include one object for each distinct point identified in the Predicted Review (Step 1).
    // If no points were raised, return an empty array: []
    {{
      "id": "PRED-POINT-1",
      "description": "[Extract the original description of the first point from <InputContext><PredictedReview><Content>]",
      "change_category": "[Assign the category code (e.g., E.1.1, F.2) from <ReferenceData><ChangeCategoryDefinitions>]",
      "severity_score": "[Assign the score (1-10) based on <ReferenceData><SeverityScale>]"
    }},
    {{
      "id": "PRED-POINT-2",
      "description": "[Extract the original description of the second point from <InputContext><PredictedReview><Content>]",
      "change_category": "[Assign the category code (e.g., E.1.1, F.2) from <ReferenceData><ChangeCategoryDefinitions>]",
      "severity_score": "[Assign the score (1-10) based on <ReferenceData><SeverityScale>]"
    }}
    // ... continue for all identified predicted points ...
  ],
  "gt_points": [
    // Include one object for each ground truth change listed in <InputContext><GroundTruthChanges><Content>.
    // Maintain the original order from the input.
    {{
      "id": "[Copy ID from the corresponding Ground Truth Change input, e.g., GT-CHANGE-1]",
      "description": "[Copy description from the corresponding Ground Truth Change input]",
      "change_category": "[Copy category from the corresponding Ground Truth Change input]",
      "hit": "[YES or NO - Based on Step 2 analysis]",
      "hit_by": "[PRED-POINT-ID if Hit is YES, otherwise N/A]"
    }},
    {{
      "id": "[Copy ID from the corresponding Ground Truth Change input, e.g., GT-CHANGE-2]",
      "description": "[Copy description from the corresponding Ground Truth Change input]",
      "change_category": "[Copy category from the corresponding Ground Truth Change input]",
      "hit": "[YES or NO - Based on Step 2 analysis]",
      "hit_by": "[PRED-POINT-ID if Hit is YES, otherwise N/A]"
    }}
    // ... continue for all Ground Truth Changes listed in the input ...
  ]
}}
"""

def load_jsonl(path):
    with open(path, "r") as f:
        data = [json.loads(line) for line in f.readlines()]
    return data


def save_jsonl(data, path):
    with open(path, "w") as f:
        for item in data:
            f.write(json.dumps(item) + "\n")


def save_json(data, path):
    with open(path, "w") as f:
        json.dump(data, f, indent=4, ensure_ascii=False)

def load_json(path):
    with open(path, "r") as f:
        data = json.load(f)
    return data

def create_clean_pr_prompt(item):
    instance = item['instance']
    pred = item['pred']
    pr_title = instance["pr_title"]
    pr_statement = instance["pr_statement"]
    # For clean PRs, the ground truth is simply that it's clean.
    # We don't need the full timeline here, just the confirmation.
    # pr_timeline = instance["pr_timeline"] 
    pred_review = pred["review"]
    
    prompt = EVAL_CLEAN_PROMPT.format(
        pr_title=pr_title,
        pr_statement=pr_statement,
        pred_review=pred_review,
    )
    
    output_structure = {
        "type": "json_schema",
        "json_schema": {
            "name": "evaluation",
            "strict": True,
            "schema": {
                "type": "object",
                "properties": {
                    "identified_as_good": {"type": "string"},
                    "pred_points": {
                        "type": "array", 
                        "items": {
                            "type": "object", 
                            "properties": {
                                "id": {"type": "string"},
                                "description": {"type": "string"},
                                "change_category": {"type": "string"},
                                "severity_score": {"type": "integer"},
                            },
                            "required": ["id", "description", "change_category", "severity_score"],
                            "additionalProperties": False
                        }
                    }
                },
                "required": ["identified_as_good", "pred_points"],
                "additionalProperties": False
            }
        }
    }
        
    return prompt, output_structure
    

def verify_clean_pr_answer(result, logger):
    if result is None:
        logger.error("Failed to get parsing response from model.")
        return None
    
    try:
        result = json.loads(result)
        # --- Start Validation for Clean Task Output --- 
        if "identified_as_good" not in result:
            logger.error(f"Missing 'identified_as_good' field: {result}")
            return None
            
        if result["identified_as_good"] not in ["YES", "NO"]:
            logger.error(f"Invalid value for 'identified_as_good': '{result['identified_as_good']}'. Must be 'YES' or 'NO'. JSON: {result}")
            return None

        if "pred_points" not in result:
            logger.error(f"Missing 'pred_points' field: {result}")
            return None
        
        if not isinstance(result['pred_points'], list):
            logger.error(f"'pred_points' field is not a list: {result}")
            return None
            
        # Validate predicted issues structure if any exist
        for pred_point in result['pred_points']:
            point_id = pred_point['id']
            if not point_id.startswith("PRED-POINT-"):
                 logger.warning(f"Unexpected format for issue ID '{point_id}'. Expected 'PRED-POINT-X'.")
                 # Allow processing but log a warning
            if "description" not in pred_point:
                logger.error(f"Missing 'description' field for issue {point_id}: {result}")
                return None
            if "severity_score" not in pred_point:
                logger.error(f"Missing 'severity_score' field for issue {point_id}: {result}")
                return None
            
            # Ensure severity scores are integers
            try:
                pred_point['severity_score'] = int(pred_point['severity_score'])
            except ValueError:
                 logger.error(f"'severity_score' is not an integer for issue {point_id}: {result}")
                 return None
            except TypeError:
                 logger.error(f"'severity_score' is not a number for issue {point_id}: {result}")
                 return None
             
            pred_point['change_category'] = pred_point['change_category'].split(" ")[0]
        # --- End Validation for Clean Task Output --- 
        
        # Return the structure directly as parsed
        return result
        
    except Exception as e:
        # import pdb; pdb.set_trace()
        logger.error(f"Unexpected error parsing clean task answer: {e}")
        return None


def create_change_pr_prompt(item):
    instance = item['instance']
    pred = item['pred']
    pr_title = instance["pr_title"]
    pr_statement = instance["pr_statement"]
    pr_timeline = instance["pr_timeline"]
    pred_review = pred["review"]
    
    changes_description = []
    for i, change in enumerate(instance['changes']):
        changes_description.append(
            f"        <Ground Truth Change GT-POINT-{i+1}>\n"
            f"            Change Category: {change['change_type']} {CHANGE_TYPE_TEXT_MAP[change['change_type']]}\n"
            f"            Change Description: {change['change_discussion']['discussion_summary']}\n"
            f"            Change Code Snippet: {change['change_introducing']['code_snippet']}\n"
            f"        </Ground Truth Change GT-POINT-{i+1}>\n"
        )
    
    # Extract ground truth review content from timeline
    ground_truth_reviews = []
    for item in pr_timeline:
        if item['type'] == 'description':
            continue  
        elif item['type'] == 'comment':
            ground_truth_reviews.append(f"<Start of Comment>\nTime: {item['created_at']}\nAuthor: {item['user']}\nComment: {item['body']}\n<End of Comment>")
        elif item['type'] == 'review_comment':
            reply_comments = ""
            for comment in item['reply']:
                reply_comments += f"<Start of Sub Review Comment>\nTime: {comment['created_at']}\nAuthor: {comment['user']}\nComment: {comment['body']}\n<End of Sub Review Comment>\n"
            ground_truth_reviews.append(f"<Start of Review Comment>\n<Start of Related Diff Hunk>\nFile: {item['path']}\n{item['diff_hunk']}\n<End of Related Diff Hunk>\n{reply_comments}<End of Review Comment>")
        elif item['type'] == 'commit':            
            ground_truth_reviews.append(f"<Start of Commit>\nTime: {item['date']}\nSHA: {item['sha']}\nAuthor: {item['author']}\nMessage: {item['message']}\n<End of Commit>")
        elif item['type'] == 'review':
            ground_truth_reviews.append(f"<Start of Review>\nTime: {item['created_at']}\nAuthor: {item['user']}\nReview: {item['body']}\n<End of Review>")
    
    assert len(ground_truth_reviews) > 0, "No ground truth review content available."
    
    ground_truth_reviews = "\n".join(ground_truth_reviews)
    changes_description = "\n".join(changes_description)

    prompt = EVAL_CHANGE_PROMPT.format(
        pr_title=pr_title,
        pr_statement=pr_statement,
        changes_description=changes_description,
        ground_truth_reviews=ground_truth_reviews,
        pred_review=pred_review
    )
    
    output_structure = {
        "type": "json_schema",
        "json_schema": {
            "name": "evaluation",
            "strict": True,
            "schema": {
                "type": "object",
                "properties": {
                    "identified_as_good": {"type": "string"},
                    "pred_points": {
                        "type": "array", 
                        "items": {
                            "type": "object", 
                            "properties": {
                                "id": {"type": "string"},
                                "description": {"type": "string"},
                                "change_category": {"type": "string"},
                                "severity_score": {"type": "integer"},
                            },
                            "required": ["id", "description", "change_category", "severity_score"],
                            "additionalProperties": False
                        }
                    },
                    "gt_points": {
                        "type": "array", 
                        "items": {
                            "type": "object", 
                            "properties": {
                                "id": {"type": "string"},
                                "description": {"type": "string"},
                                "change_category": {"type": "string"},
                                "hit": {"type": "string"},
                                "hit_by": {"type": "string"},
                            },
                            "required": ["id", "description", "change_category", "hit", "hit_by"],
                            "additionalProperties": False
                        }
                    }
                },
                "required": ["identified_as_good", "pred_points", "gt_points"],
                "additionalProperties": False
            }
        }
    }
    
    return prompt, output_structure


def verify_change_pr_answer(result, instance, logger):
    if result is None:
        logger.error("Failed to get parsing response from model for change task.") # Specific log
        return None
    
    # if "```json" in result:
    #     result = result.split("```json")[1].split("```")[0]
    
    try:
        result = json.loads(result)
        # --- Start Validation for Change Task Output --- 
        if "identified_as_good" not in result:
            logger.error(f"Missing 'identified_as_good' field: {result}")
            return None
            
        if result["identified_as_good"] not in ["YES", "NO"]:
            logger.error(f"Invalid value for 'identified_as_good': '{result['identified_as_good']}'. Must be 'YES' or 'NO'. JSON: {result}")
            return None

        # Validate required fields (pred_points and gt_points)
        if "pred_points" not in result:
            logger.error(f"Missing 'pred_points' field: {result}")
            return None
            
        if "gt_points" not in result:
            logger.error(f"Missing 'gt_points' field: {result}")
            return None

        if not isinstance(result['pred_points'], list):
            logger.error(f"'pred_points' field is not a list: {result}")
            return None
        if not isinstance(result['gt_points'], list):
            logger.error(f"'gt_points' field is not a list: {result}")
            return None
        
        # Validate predicted changes structure
        for point_info in result['pred_points']:
            if not point_info['id'].startswith("PRED-POINT-"):
                logger.warning(f"Unexpected format for predicted change ID '{point_info['id']}'. Expected 'PRED-POINT-X'.")
            if "description" not in point_info:
                logger.error(f"Missing 'description' field for point {point_info['id']}: {result}")
                return None
            if "severity_score" not in point_info:
                logger.error(f"Missing 'severity_score' field for point {point_info['id']}: {result}")
                return None
            if "change_category" not in point_info:
                logger.error(f"Missing 'change_category' field for point {point_info['id']}: {result}")
                return None
            # Ensure severity scores are integers
            try:
                point_info['severity_score'] = int(point_info['severity_score'])
            except (ValueError, TypeError):
                 logger.error(f"'severity_score' is not an integer for point {point_info['id']}: {result}")
                 return None
            
        # Validate ground truth changes structure
        gt_ids_from_instance = {f"GT-POINT-{i+1}" for i in range(len(instance.get('changes', [])))} # Handle case where instance might not have changes?
        for gt_info in result['gt_points']:
            gt_id = gt_info['id']
            if gt_id not in gt_ids_from_instance:
                 logger.error(f"Invalid or unexpected ground truth change ID '{gt_id}': {result}")
                 return None
            if "change_category" not in gt_info:
                logger.error(f"Missing 'change_category' field for {gt_id}: {result}")
                return None
            if "description" not in gt_info:
                logger.error(f"Missing 'description' field for {gt_id}: {result}")
                return None
            if "hit" not in gt_info or gt_info['hit'] not in ["YES", "NO"]:
                logger.error(f"Missing or invalid 'hit' field ('{gt_info.get('hit')}') for {gt_id}: {result}")
                return None
            if "hit_by" not in gt_info:
                logger.error(f"Missing 'hit_by' field for {gt_id}: {result}")
                return None
            if gt_info['hit'] == "NO" and gt_info['hit_by'] != "N/A":
                 logger.warning(f"'hit_by' field is '{gt_info['hit_by']}' but 'hit' is NO for {gt_id}. Setting hit_by to N/A.")
                 gt_info['hit_by'] = "N/A"
            elif gt_info['hit'] == "YES" and not gt_info['hit_by'].startswith("PRED-POINT-"):
                 logger.error(f"Invalid 'hit_by' value ('{gt_info['hit_by']}') when hit is YES for {gt_id}: {result}")
                 return None
        # --- End Validation for Change Task Output ---
        
        assert len(result['gt_points']) == len(instance['changes'])
        # Add change types back to gt_points
        changes_dict = {f"GT-POINT-{i+1}": change for i, change in enumerate(instance['changes'])}
        for idx, gt_info in enumerate(result['gt_points']):
            gt_change_id = result['gt_points'][idx]['id']
            result['gt_points'][idx]['change_category'] = changes_dict[gt_change_id]['change_type']
        
        for pred_point in result['pred_points']:
            pred_point['change_category'] = pred_point['change_category'].split(" ")[0]
        # Return the structure directly as parsed (no transformation needed anymore)
        return result

    except Exception as e:
        # import pdb; pdb.set_trace()
        logger.error(f"Unexpected error parsing change task answer: {e}")
        return None


def create_messages(message, system_message=None):
    messages = [{"role": "system", "content": system_message}] if system_message is not None else []
    messages.append({"role": "user", "content": message})
    return messages

def fix_change_type(instance):
    for change in instance['changes']:
        change['change_type'] = change['change_type'].split(" ")[0]
    return instance

def evaluate_one(args, item, logger):
    # system_message = "You are a helpful assistant."
    item['pred']['review'] = item['pred']['review'].replace("â€™", "'")
    item['instance'] = fix_change_type(item['instance'])
    system_message = None
    # import pdb; pdb.set_trace()
    change_introduced = item['instance']['change_introduced']
    if change_introduced:
        prompt, output_structure = create_change_pr_prompt(item)
    else:
        prompt, output_structure = create_clean_pr_prompt(item)
    instance_id = item['instance']['instance_id']
    messages = create_messages(
        message=prompt,
        system_message=system_message
    )
    # logger.debug(f"Sending request for instance {instance_id} ...")
    # logger.debug(f"Prompt: \n{prompt}")
    result = None # Initialize result
    response = None # Initialize response
    for i in range(3):
        # import pdb; pdb.set_trace()
        response = run_chat(
            model=args.model,
            messages=messages,
            temperature=args.temperature,
            max_tokens=args.max_tokens,
            response_format=output_structure,
            max_retries=5
        )
        if response is None:
            logger.warning(f"Failed to get response for instance {instance_id} (Attempt {i+1})")
            continue # Try again
        # logger.debug(f"Received response for instance {instance_id}: {response}")

        # Determine which parser to use based on whether changes were introduced
        if change_introduced:
            result = verify_change_pr_answer(response, item['instance'], logger)
        else:
            result = verify_clean_pr_answer(response, logger)

        if result is not None:
            # logger.debug(f"Successfully parsed answer for instance {instance_id} (Attempt {i+1})")
            break # Exit loop if parsing is successful
        else:
            logger.warning(f"Failed to parse answer for instance {instance_id} (Attempt {i+1}) and retrying...")
            # Optionally add the failed response to messages if retrying with context is desired
            # messages.append({"role": "assistant", "content": response})
            # messages.append({"role": "user", "content": "Parsing failed. Please provide the output strictly in the specified JSON format."})


    if result is None:
        logger.error(f"Failed to get and parse answer for instance {instance_id} after 3 attempts.")
        # Store the last failed response if needed for debugging
        # return {"instance_id": instance_id, "error": "Parsing failed", "last_response": response}
        return None # Return None if all attempts fail

    # Append the successful assistant response
    messages.append({"role": "assistant", "content": response})

    # Process result based on task type
    if change_introduced:
        # Calculate hit rate
        hit_count = sum(1 for gt_info in result['gt_points'] if gt_info['hit'].lower() == 'yes')
        total_count = len(result['gt_points'])

        return {
            "instance_id": instance_id,
            "change_introduced": True,
            "hit": hit_count,
            "total": total_count,
            "pred_points": result['pred_points'],
            "gt_points": result['gt_points'],
            "identified_as_good": result['identified_as_good'],
            "response": response, # Store the successful response
            "review": item['pred']['review'],
            "messages": messages # Store the conversation history
        }
    else: # Clean PR case
         return {
            "instance_id": instance_id,
            "change_introduced": False,
            "identified_as_good": result['identified_as_good'],
            "pred_points": result['pred_points'],
            "response": response, # Store the successful response
            "review": item['pred']['review'],
            "messages": messages # Store the conversation history
        }


def analyze_result(results):
    # Helper function for P/R/F1 calculation
    def calculate_prf1(tp, fp, fn):
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        # Ensure results are serializable if they end up in JSON
        precision = float(precision) if not isinstance(precision, (int, float)) else precision
        recall = float(recall) if not isinstance(recall, (int, float)) else recall
        f1 = float(f1) if not isinstance(f1, (int, float)) else f1
        return round(precision, 4), round(recall, 4), round(f1, 4)
        
    def analyze_change_results(input_results):
        this_analyzed_results = []

        for res in input_results:
            # Aggregate FP (predicted issues in this instance that didn't hit any GT issue)
            analyzed_result = {"instance_id": res['instance_id']}
            pred_points = res['pred_points']
            gt_points = res['gt_points']
            
            if len(pred_points) == 0 and len(gt_points) == 0:
                continue
            
            pred_point_ids = set([pred_point['id'] for pred_point in pred_points])
            hit_by_ids = set()
            for gt_info in gt_points:
                hit_by = gt_info['hit_by']
                if hit_by in pred_point_ids:
                    hit_by_ids.add(hit_by)
            
            analyzed_result['tp'] = len(hit_by_ids)
            analyzed_result['fp'] = len(pred_point_ids - hit_by_ids)
            analyzed_result['fn'] = len(gt_points) - len(hit_by_ids)
            
            precision, recall, f1 = calculate_prf1(analyzed_result['tp'], analyzed_result['fp'], analyzed_result['fn'])
            analyzed_result['precision'] = precision
            analyzed_result['recall'] = recall
            analyzed_result['f1'] = f1
            
            analyzed_result['total_pred_points'] = len(pred_points)
            analyzed_result['total_gt_points'] = len(gt_points)
            analyzed_result['severity_list'] = [d['severity_score'] for d in pred_points]
            analyzed_result['severity_avg'] = np.mean(analyzed_result['severity_list']) if analyzed_result['severity_list'] else 0
            analyzed_result['identified_as_good'] = True if res['identified_as_good'].lower() == 'yes' or len(pred_points) == 0 else False
            analyzed_result['pred_types'] = [pred_point['change_category'] for pred_point in pred_points]
            analyzed_result['gt_types'] = [gt_point['change_category'] for gt_point in gt_points]
            # # Collect identified as good info (all pred severities <= 3 or no issues)
            # all_low_severity = all(s <= 3 for s in analyzed_result['severity_list'])
            # analyzed_result['identified_as_good'] = 1 if all_low_severity else 0
            
            this_analyzed_results.append(analyzed_result)
        
        tp_total = sum([res['tp'] for res in this_analyzed_results])
        fp_total = sum([res['fp'] for res in this_analyzed_results])
        fn_total = sum([res['fn'] for res in this_analyzed_results])
        precision_total, recall_total, f1_total = calculate_prf1(tp_total, fp_total, fn_total)
        
        severity_total_list = []
        for res in this_analyzed_results:
            severity_total_list.extend(res['severity_list'])
        severity_total = np.mean(severity_total_list) if severity_total_list else 0
        
        pred_types_count = {}
        for res in this_analyzed_results:
            for pred_type in res['pred_types']:
                pred_types_count[pred_type] = pred_types_count.get(pred_type, 0) + 1
        
        gt_types_count = {}
        for res in this_analyzed_results:
            for gt_type in res['gt_types']:
                gt_types_count[gt_type] = gt_types_count.get(gt_type, 0) + 1
        
        report = {
            "precision_avg": np.mean([res['precision'] for res in this_analyzed_results]),
            "recall_avg": np.mean([res['recall'] for res in this_analyzed_results]),
            "f1_avg": np.mean([res['f1'] for res in this_analyzed_results]),
            "severity_avg": np.mean([res['severity_avg'] for res in this_analyzed_results]),
            "precision_total": precision_total,
            "recall_total": recall_total,
            "f1_total": f1_total,
            "severity_total": severity_total,
            "severity_list": severity_total_list,
            "identified_as_good_rate": np.mean([res['identified_as_good'] for res in this_analyzed_results]),
            "identified_as_good": sum([res['identified_as_good'] for res in this_analyzed_results]),
            "tp_total": tp_total,
            "fp_total": fp_total,
            "fn_total": fn_total,
            "pred_points_total": sum([res['total_pred_points'] for res in this_analyzed_results]),
            "gt_points_total": sum([res['total_gt_points'] for res in this_analyzed_results]),
            "pred_points_avg": sum([res['total_pred_points'] for res in this_analyzed_results]) / len(input_results),
            "gt_points_avg": sum([res['total_gt_points'] for res in this_analyzed_results]) / len(input_results),
            "pred_types_count": pred_types_count,
            "gt_types_count": gt_types_count,
            "count": len(this_analyzed_results)
        }
        return report, this_analyzed_results
    
    def analyze_clean_results(input_results):
        this_analyzed_results = []

        for res in input_results:
            analyze_result = {"instance_id": res['instance_id']}
            pred_points = res['pred_points']
            instance_severities = [point['severity_score'] for point in pred_points] 
            analyze_result['total_pred_points'] = len(pred_points)
            analyze_result['severity_list'] = instance_severities
            analyze_result['severity_avg'] = np.mean(instance_severities) if instance_severities else 0
            analyze_result['identified_as_good'] = True if res['identified_as_good'].lower() == 'yes' or len(pred_points) == 0 else False
            pred_types = [point['change_category'] for point in pred_points]
            analyze_result['pred_types'] = pred_types
            this_analyzed_results.append(analyze_result)

        pred_types_count = {}
        for res in this_analyzed_results:
            for pred_type in res['pred_types']:
                pred_types_count[pred_type] = pred_types_count.get(pred_type, 0) + 1

        all_severity_scores = []
        for res in this_analyzed_results:
            all_severity_scores.extend(res['severity_list'])
    
        report = {
            "pred_points_total": sum([res['total_pred_points'] for res in this_analyzed_results]),
            "pred_points_avg": np.mean([res['total_pred_points'] for res in this_analyzed_results]),
            "severity_avg": np.mean([res['severity_avg'] for res in this_analyzed_results]),
            "severity_list": all_severity_scores,
            "identified_as_good_rate": np.mean([res['identified_as_good'] for res in this_analyzed_results]),
            "identified_as_good": sum([res['identified_as_good'] for res in this_analyzed_results]),
            "pred_types_count": pred_types_count,
            "count": len(this_analyzed_results),
        }
        return report, this_analyzed_results

    def analyze_changetype_results(input_results, change_types):
        this_analyzed_results = []

        change_type_severity_list = []
        for res in input_results:
            tp, fp, fn = 0, 0, 0
            analyze_result = {"instance_id": res['instance_id']}
            pred_points = res['pred_points']
            change_type_pred_points_size = 0
            change_type_gt_points_size = 0
            if res['change_introduced']:
                pred_point_ids = set([pred_point['id'] for pred_point in pred_points])
                hit_by_ids = set([gt_point['hit_by'] for gt_point in res['gt_points']])
                for gt_point in res['gt_points']:
                    # assert gt_point['change_category'] in CHANGE_TYPE_TEXT_MAP, f"Invalid change category: {gt_point['change_category']}"
                    if gt_point['change_category'] in change_types:
                        change_type_gt_points_size += 1
                        if gt_point['hit'].lower() == "no":
                            fn += 1
                for pred_point in pred_points:
                    # assert pred_point['change_category'] in CHANGE_TYPE_TEXT_MAP, f"Invalid change category: {pred_point['change_category']}"
                    if pred_point['change_category'] in change_types:
                        change_type_pred_points_size += 1
                        change_type_severity_list.append(pred_point['severity_score'])
                        if pred_point['id'] in hit_by_ids:
                            tp += 1
                        else:
                            fp += 1
            else:
                for pred_point in pred_points:
                    # assert pred_point['change_category'] in CHANGE_TYPE_TEXT_MAP, f"Invalid change category: {pred_point['change_category']}"
                    if pred_point['change_category'] in change_types:
                        change_type_pred_points_size += 1
                        change_type_severity_list.append(pred_point['severity_score'])
                        fp += 1
                        
            analyze_result['pred_points_size'] = change_type_pred_points_size
            analyze_result['gt_points_size'] = change_type_gt_points_size
            analyze_result['tp'] = tp
            analyze_result['fp'] = fp
            analyze_result['fn'] = fn
            this_analyzed_results.append(analyze_result)
    
        tp_total = sum([res['tp'] for res in this_analyzed_results])
        fp_total = sum([res['fp'] for res in this_analyzed_results])
        fn_total = sum([res['fn'] for res in this_analyzed_results])
        precision, recall, f1 = calculate_prf1(tp_total, fp_total, fn_total)
        report = {
            "precision": precision,
            "recall": recall,
            "f1": f1,
            "tp": tp_total,
            "fp": fp_total,
            "fn": fn_total,
            "avg_pred_points": sum([res['pred_points_size'] for res in this_analyzed_results]) / len(this_analyzed_results),
            "avg_gt_points": sum([res['gt_points_size'] for res in this_analyzed_results]) / len(this_analyzed_results),
            "avg_severity": np.mean(change_type_severity_list),
            "count": len(this_analyzed_results)
        }
        return report, this_analyzed_results
    
    valid_results = [result for result in results if result is not None] # Simplified check

    analysis_results = {'submit': len(results), 'valid': len(valid_results), 'error': len(results) - len(valid_results)}

    # Separate results for change and clean tasks
    change_results = [res for res in valid_results if res['change_introduced'] == True] # Explicit check for True
    clean_results = [res for res in valid_results if res['change_introduced'] == False] # Explicit check for False

    change_report, change_analyzed_results = analyze_change_results(change_results)
    clean_report, clean_analyzed_results = analyze_clean_results(clean_results)

    overall_tp = change_report['count'] - change_report['identified_as_good']
    overall_tn = clean_report['identified_as_good']
    overall_fp = clean_report['count'] - clean_report['identified_as_good']
    overall_fn = change_report['identified_as_good']
    overall_precision, overall_recall, overall_f1 = calculate_prf1(overall_tp, overall_fp, overall_fn)
    overall_accuracy = (overall_tp + overall_tn) / (overall_tp + overall_tn + overall_fp + overall_fn) if (overall_tp + overall_tn + overall_fp + overall_fn) > 0 else 0
    
    tp_point = change_report['tp_total']
    fp_point = change_report['fp_total'] + clean_report['pred_points_total']
    fn_point = change_report['fn_total']
    
    precision_point, recall_point, f1_point = calculate_prf1(tp_point, fp_point, fn_point)
    
    overall_report = {
        "precision": overall_precision,
        "recall": overall_recall,
        "f1": overall_f1,
        "accuracy": overall_accuracy,
        "precision_point": precision_point,
        "recall_point": recall_point,
        "f1_point": f1_point,
        "tp": overall_tp,
        "fp": overall_fp,
        "fn": overall_fn,
        "tn": overall_tn,
        "tp_point": tp_point,
        "fp_point": fp_point,
        "fn_point": fn_point,
        "avg_pred_points": (change_report['pred_points_total'] + clean_report['pred_points_total']) / len(valid_results),
        "avg_pred_severity": np.mean(change_report['severity_list'] + clean_report['severity_list']),
    }
    change_report.pop('severity_list')
    clean_report.pop('severity_list')
    analysis_results['overall'] = overall_report
    analysis_results['change'] = change_report
    analysis_results['clean'] = clean_report
    analysis_results['change_by_type'] = {}
    
    change_report, _ = analyze_changetype_results(valid_results, 
                                            [change_type for change_type in CHANGE_TYPE_TEXT_MAP if change_type.startswith('F')])
    analysis_results['change_by_F'] = change_report
    change_report, _ = analyze_changetype_results(valid_results, 
                                            [change_type for change_type in CHANGE_TYPE_TEXT_MAP if change_type.startswith('E')])
    analysis_results['change_by_E'] = change_report
    
    for change_type in CHANGE_TYPE_TEXT_MAP:
        change_report, change_analyzed_results = analyze_changetype_results(valid_results, [change_type])
        analysis_results['change_by_type'][change_type] = change_report
    
    # for change_type in CHANGE_TYPE_TEXT_MAP:
    #     change_report, change_analyzed_results = analyze_change_results(change_results, change_type=change_type)
    #     analysis_results['change_by_type'][change_type] = change_report
    
    return analysis_results


def evaluate(args):
    random.seed(42)
    dataset = load_jsonl(args.dataset_file)
    dataset_dict = {item['instance_id']: item for item in dataset}
    predictions = load_jsonl(args.pred_file)
    random.shuffle(predictions)
    # predictions = predictions[:10]
    
    os.makedirs(os.path.dirname(args.output_file), exist_ok=True)
    
    logger.add(args.output_file + ".log", rotation="10 MB", encoding="utf-8")
    
    logger.info("="*100)
    logger.info(f"args: {args}")
    logger.info("="*100)
    logger.info(f"Evaluating {len(predictions)} instances")
    logger.info(f"Dataset File: {args.dataset_file}")
    logger.info(f"Model: {args.model}")
    logger.info(f"Output File: {args.output_file}")
    logger.info("="*100)
    
    tasks = [{'pred': pred, 'instance': dataset_dict[pred['instance_id']]} for pred in predictions]
    
    cache_file = args.output_file + ".tmp.jsonl"
    if os.path.exists(cache_file) and not args.overwrite:
        results = load_jsonl(cache_file)
        logger.info(f"Loaded {len(results)} results from cache file")
        finished_instance_ids = [result['instance_id'] for result in results]
        tasks = [task for task in tasks if task['pred']['instance_id'] not in finished_instance_ids]
    else:
        with open(cache_file, "w") as f:
            f.write("")
        results = []
    
    file_lock = threading.Lock()
    def process_item(item):
        result = evaluate_one(args, item, logger)
        if result is not None:
            with file_lock:
                with open(cache_file, "a") as f:
                    f.write(json.dumps(result) + "\n")

    with ThreadPoolExecutor(max_workers=args.num_threads) as executor:
        futures = [executor.submit(process_item, item) for item in tasks]
        for future in tqdm(as_completed(futures), total=len(tasks), desc="Processing"):
            future.result()
    
    # for item in tqdm(tasks, total=len(tasks), desc="Processing"):
    #     process_item(item)
    
    results = load_jsonl(cache_file)
    
    analysis_results = analyze_result(results)
    logger.info(f"Analysis Results: \n{json.dumps(analysis_results, indent=4)}")
    
    save_result = {
        "analysis_results": analysis_results,
        "details": results
    }
    
    save_json(save_result, args.output_file)
    logger.info(f"Finished processing {len(dataset)} instances")
    logger.info(f"Output File: {args.output_file}")



if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset-file", type=str, help="Path to dataset file")
    parser.add_argument("--pred-file", type=str, help="Path to predictions file")
    parser.add_argument("--output-file", type=str, help="Path to output file")
    parser.add_argument("--model", type=str, help="Model name")
    parser.add_argument("--instance-ids", nargs="+", help="Instance ids")
    parser.add_argument("--ignore-ids", nargs="+", help="Ignore instance ids")
    parser.add_argument("--num-threads", default=1, type=int, help="Number of threads")
    parser.add_argument("--temperature", default=0.0, type=float, help="Temperature")
    parser.add_argument("--max-tokens", default=8192, type=int, help="Max tokens")
    parser.add_argument("--overwrite", action="store_true", help="Overwrite cache file")
    args = parser.parse_args()

    # evaluate
    evaluate(args)
    
# TODO: Add actual modified location and code snippet